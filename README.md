Here is a professionally structured, GitHub-ready `README.md` file based on your project specifications. I have optimized the formatting for readability, added syntax highlighting, and included a clean directory tree.

---

# README.md

# ğŸ­ Multi-Portal Tender Scraper (Production Version)

An enterprise-grade, fault-tolerant, multi-process tender scraping system. Designed for high-volume extraction from government e-procurement portals with integrated AI filtering, UPSERT database logic, and automated reporting.

## ğŸš€ Overview

This system automates the end-to-end lifecycle of tender data collection:

* **Multi-Portal Support:** Simultaneous scraping of diverse government portals.
* **AI-Driven Intelligence:** Uses Mistral LLM to filter and classify tender relevance.
* **Robust Persistence:** SQLite backend with UPSERT logic to prevent duplicates.
* **Resilience:** Built-in resume capability, session refreshing, and isolated error handling.
* **Reporting:** Automatic generation of Excel mirrors and consolidated performance analytics.

---

## ğŸ— Architecture Overview

The system operates via three core modules:

1. **`production_orchestrator.py`**: The "Brain." Manages multiprocessing, process isolation, and staggered starts.
2. **`production_portal_scraper.py`**: The "Worker." Handles Phase 1 (Pagination), AI Filtering, and Phase 2 (Deep Extraction).
3. **`data_aggregation.py`**: The "Analyst." Compiles cross-portal stats and generates the final business reports.

### Workflow Logic

```text
production_orchestrator.py
        â”‚
        â”œâ”€â”€ Multiprocessing (Isolated Environments)
        â”‚
        â”œâ”€â”€ production_portal_scraper.py
        â”‚       â”œâ”€â”€ Phase 1: Unlimited pagination & raw extraction
        â”‚       â”œâ”€â”€ AI Filtering: Title classification via Mistral
        â”‚       â”œâ”€â”€ Phase 2: Work Description & Deep Link extraction
        â”‚       â””â”€â”€ Storage: UPSERT to SQLite & Local Excel Mirrors
        â”‚
        â””â”€â”€ data_aggregation.py
                â””â”€â”€ Consolidated Excel Reports & Performance Metrics

```

---

## ğŸŒ Supported Portals

| Portal ID | Name | Notes |
| --- | --- | --- |
| **WB** | West Bengal | Standard NIC portal architecture |
| **BHEL** | BHEL | Specialized enterprise portal |
| **COAL** | Coal India | NIC-based structure |
| **NTPC** | NTPC | Includes custom alert dialog handling |

---

## ğŸ—„ Database Schema

The system utilizes `database/multi_portal_tenders.db` with the following core structure:

### 1. `tenders` Table

* `identity_hash`: Unique SHA-256 hash to prevent duplicates.
* `portal_id`: Source identifier.
* `ai_filtered`: Boolean flag for relevance.
* `phase2_status`: Tracking for deep extraction progress.

### 2. Supporting Tables

* **`scraping_metadata`**: Stores pagination states for auto-resume.
* **`failed_urls`**: Error logging for Phase 2 retries.
* **`portal_execution_log`**: Audit trail (Start/End times, page counts, error rates).

---

## âš™ï¸ Installation

### 1. Clone & Setup

```bash
git clone https://github.com/your-username/multi-portal-tender-scraper.git
cd multi-portal-tender-scraper
python -m venv venv
source venv/bin/activate  # Linux/macOS
# OR
venv\Scripts\activate     # Windows

```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
playwright install chromium

```

### 3. Environment Variables

Set your Mistral API key for the AI filtering module:

```powershell
# Windows
setx MISTRAL_API_KEY "your_api_key_here"

# Linux/macOS
export MISTRAL_API_KEY="your_api_key_here"

```

---

## â–¶ï¸ Running the System

### Full Production Run

To launch all scrapers concurrently with isolated logging:

```bash
python production_orchestrator.py

```

### Generate Reports

To aggregate data from the database into a polished Excel file:

```bash
python data_aggregation.py

```

---

## ğŸ§  AI Filtering (Mistral)

The system utilizes the `mistral-large-latest` model to process tender titles in batches:

* **Batch Size:** 50 titles per request.
* **Logic:** Classifies tenders as "Meaningful" or "Unmeaningful" based on project scope.
* **Cache:** Each portal maintains an isolated AI cache to reduce API costs.

---

## ğŸ›¡ Production Safety Features

* âœ… **SQLite Transaction Safety:** Prevents DB corruption during concurrent writes.
* âœ… **Session Refresh:** Automatically restarts browser contexts every 10 detailed scrapes.
* âœ… **Staggered Start:** 5-second delay between portal launches to prevent CPU spikes.
* âœ… **Auto-Resume:** Detects previous crashes and picks up from the last scraped page.

---

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ production_orchestrator.py   # Main entry point
â”œâ”€â”€ production_portal_scraper.py  # Core scraping logic
â”œâ”€â”€ data_aggregation.py           # Reporting & Analytics
â”œâ”€â”€ database/
â”‚   â””â”€â”€ multi_portal_tenders.db   # Central SQLite storage
â”œâ”€â”€ portals/                      # Portal-specific assets
â”‚   â”œâ”€â”€ WB/
â”‚   â”œâ”€â”€ BHEL/
â”‚   â””â”€â”€ NTPC/
â”‚       â”œâ”€â”€ logs/                 # Rotation-based log files
â”‚       â”œâ”€â”€ excel_mirrors/        # Live-updated Excel backups
â”‚       â””â”€â”€ checkpoints/          # Resume state files
â””â”€â”€ README.md

```

---

## ğŸ† Comparison: Script vs. Production System

| Feature | Typical Script | This System |
| --- | --- | --- |
| **Architecture** | Single-threaded | Isolated Multiprocessing |
| **Interruption** | Data loss / Restart | Full Auto-Resume |
| **Deduplication** | Manual/None | SQL UPSERT Logic |
| **Error Handling** | Generic Try/Except | Detailed Error Tracking Table |
| **Scaling** | Limited | 5,000+ tenders per portal |

---

## ğŸ‘¨â€ğŸ’» Author

**Multi-Portal Scraper System** Built for scalable, fault-tolerant government tender extraction.

---

### Would you like me to generate the `requirements.txt` file or a `Dockerfile` to containerize this setup?
